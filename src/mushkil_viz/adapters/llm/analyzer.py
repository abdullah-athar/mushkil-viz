from typing import Dict, List, Optional, Any, Callable
import pandas as pd
import numpy as np
from pydantic import BaseModel, Field
from openai import OpenAI
from mushkil_viz.core.analyzers.base import BaseAnalyzer
from mushkil_viz.core.client_init import init_openai_client
import json
from scipy import stats

class AnalysisFunction(BaseModel):
    """Schema for an analysis function generated by the LLM."""
    name: str = Field(..., description="Name of the analysis function")
    description: str = Field(..., description="Description of what the analysis does")
    code: str = Field(..., description="The Python code for the analysis function")
    required_columns: List[str] = Field(..., description="List of columns required for this analysis")

class AnalysisResult(BaseModel):
    """Schema for the result of an analysis function."""
    name: str = Field(..., description="Name of the analysis that was performed")
    description: str = Field(..., description="Description of the analysis")
    result: Dict[str, Any] = Field(..., description="The results of the analysis")
    visualization_hints: Optional[Dict[str, str]] = Field(None, description="Hints for how to visualize this result")

class DatasetContext(BaseModel):
    column_types: Dict[str, List[str]] = Field(..., description="Mapping of column types to column names")
    sample_data: Dict[str, List[Any]] = Field(..., description="Sample values from each column")
    basic_stats: Dict[str, Dict[str, float]] = Field(..., description="Basic statistical measures for numeric columns")
    column_descriptions: Dict[str, str] = Field(..., description="Description of what each column represents")

class AnalysisPlan(BaseModel):
    """Schema for the complete analysis plan from the LLM."""
    functions: List[AnalysisFunction] = Field(..., description="List of analysis functions to execute")

class LLMAnalyzer(BaseAnalyzer):
    """Analyzer that uses LLMs to generate and execute data analysis functions."""
    
    def __init__(self, api_key: Optional[str] = None, api_base: Optional[str] = None):
        super().__init__()
        self.client = init_openai_client(api_key, api_base)
        self.model = "deepseek/deepseek-r1:free"
        
        self.function_schema = {
            "name": "provide_analysis_plan",
            "description": "Generate analysis functions based on the dataset context",
            "parameters": AnalysisPlan.model_json_schema()
        }
        
        self.system_prompt = """You are a data analysis expert. Your task is to generate Python functions that perform meaningful analyses on the dataset.
        For each analysis function:
        1. Give it a descriptive name
        2. Provide a clear description of what it analyzes
        3. Write efficient Pandas/NumPy code to perform the analysis
        4. List the required columns
        5. Include appropriate error handling
        6. Return results in a format suitable for visualization
        
        Focus on:
        - Statistical relationships between variables
        - Distributions and patterns
        - Aggregations and groupings
        - Temporal patterns (if time-based data exists)
        - Geographical patterns (if location data exists)
        - Key business/domain metrics
        
        The code should be production-ready and handle edge cases (nulls, outliers, etc.)."""

    def _generate_analysis_functions(self, context: DatasetContext) -> List[AnalysisFunction]:
        """Generate analysis functions based on the dataset context."""
        
        user_prompt = f"""Dataset Context:
        Column Types: {json.dumps(context.column_types, indent=2)}
        Sample Data: {json.dumps(context.sample_data, indent=2)}
        Basic Stats: {json.dumps(context.basic_stats, indent=2)}
        Column Descriptions: {json.dumps(context.column_descriptions, indent=2)}
        
        Please generate appropriate analysis functions for this dataset."""

        response = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            functions=[self.function_schema],
            function_call={"name": "provide_analysis_plan"}
        )

        function_call = response.choices[0].message.function_call
        analysis_plan = AnalysisPlan.model_validate_json(function_call.arguments)
        return analysis_plan.functions

    def _execute_analysis_function(self, df: pd.DataFrame, analysis: AnalysisFunction) -> AnalysisResult:
        """Execute a single analysis function and return its results."""
        try:
            # Create the function namespace with necessary imports
            namespace = {
                'pd': pd,
                'np': np,
                'stats': stats,
                'df': df
            }
            
            # Execute the analysis code
            exec(analysis.code, namespace)
            
            # Get the result from the last assigned variable
            result = namespace.get('result', None)
            if result is None:
                raise ValueError("Analysis function did not produce a 'result' variable")
            
            return AnalysisResult(
                name=analysis.name,
                description=analysis.description,
                result=result if isinstance(result, dict) else {'data': result},
                visualization_hints=namespace.get('visualization_hints', None)
            )
        except Exception as e:
            return AnalysisResult(
                name=analysis.name,
                description=analysis.description,
                result={'error': str(e)},
                visualization_hints=None
            )

    def analyze(self, df: pd.DataFrame) -> Dict:
        """Perform comprehensive analysis of the dataset using LLM-generated functions."""
        # First, run the parent class analysis
        base_analysis = super().analyze(df)
        
        # Prepare enhanced context for LLM
        context = DatasetContext(
            column_types=base_analysis["dataset_info"]["column_types"],
            sample_data={col: df[col].head(5).tolist() for col in df.columns},
            basic_stats=base_analysis["basic_stats"],
            column_descriptions={col: f"Column containing {col.replace('_', ' ')}" for col in df.columns}
        )
        
        # Generate analysis functions
        analysis_functions = self._generate_analysis_functions(context)
        
        # Execute each analysis function and collect results
        analysis_results = []
        for func in analysis_functions:
            # Check if required columns are present
            if all(col in df.columns for col in func.required_columns):
                result = self._execute_analysis_function(df, func)
                analysis_results.append(result.model_dump())
        
        # Combine base analysis with function results
        results = base_analysis
        results["analysis_results"] = analysis_results
        
        return results 